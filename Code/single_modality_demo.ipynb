{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import re\n",
    "from glob import glob\n",
    "from dataset_loader import create_pytorch_dataset\n",
    "import parameters\n",
    "\n",
    "window_len = parameters.window_len\n",
    "stride = parameters.stride\n",
    "fair_comparison = parameters.fair_comparison\n",
    "TOD = parameters.TOD\n",
    "forward_chunk_size = parameters.forward_chunk_size\n",
    "\n",
    "device = parameters.device\n",
    "key_frame_extraction = parameters.key_frame_extraction\n",
    "key_frame_extraction_algorithm = parameters.key_frame_extraction_algorithm\n",
    "feature_extraction = parameters.feature_extraction\n",
    "background_subtraction = parameters.background_subtraction\n",
    "background_subtraction_algorithm = parameters.background_subtraction_algorithm\n",
    "data_augmentation = parameters.data_augmentation\n",
    "anomaly_detection_model = parameters.anomaly_detection_model\n",
    "\n",
    "frame_rate_adjusted_dataset = parameters.frame_rate_adjusted_dataset\n",
    "dataset_category = parameters.dataset_category\n",
    "project_directory = parameters.project_directory\n",
    "dataset_directory = parameters.dataset_directory\n",
    "ht = parameters.ht\n",
    "wd = parameters.wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display Original Video, h5py file Video, Input Video and Reconstructed Output Video\n",
    "def display_videos(name, dset, path, vid_folder, modified_video, reconstructed_video, labels):\n",
    "\n",
    "    display_ht = 450\n",
    "    display_wd = 450\n",
    "    # Video can be 4fps / 8fps / 20fps. So each frame can be displayed for 250 ms / 125 ms / 50 ms\n",
    "    ms_per_frame = 125  # millisecond per frame\n",
    "\n",
    "    # Extract if fall folder or ADL folder\n",
    "    dir_type = re.findall(\"[a-zA-Z]+\", vid_folder)[0]\n",
    "\n",
    "    # Original Video\n",
    "    vid_location = \"{}\\Dataset\\Fall-Data\\{}\\{}\\{}\\{}\".format(\n",
    "        dataset_directory, dataset_category, dset, dir_type, vid_folder\n",
    "    )\n",
    "    vid_location = glob(vid_location + \"/*.jpg\") + glob(vid_location + \"/*.png\")\n",
    "    vid_location.sort(key=lambda var: [int(x) if x.isdigit() else x for x in re.findall(r\"[^0-9]|[0-9]+\", var)])\n",
    "    original_video = []\n",
    "    for filename in vid_location:\n",
    "        img = cv2.imread(filename, cv2.IMREAD_ANYCOLOR)\n",
    "        if img is not None:\n",
    "            original_video.append(img)\n",
    "\n",
    "    # Preprocessed Video\n",
    "    with h5py.File(path, \"r\") as hf:\n",
    "        data_dict = hf[\"{}/Processed/Split_by_video\".format(name)]\n",
    "        preprocessed_video = data_dict[vid_folder][\"Data\"][:]\n",
    "\n",
    "    # Modified Video\n",
    "    modified_video = modified_video\n",
    "\n",
    "    # Output Video.\n",
    "    # Original Shape - [batch_size, no.of windows, window_len, ht, wd].\n",
    "    # New Shape - [no.of windows, window_len, ht, wd] (batch_size is 1)\n",
    "    reconstructed_video = reconstructed_video.reshape(reconstructed_video.shape[1], window_len, ht, wd)\n",
    "    output_video = []\n",
    "    # Reconstructed Video. It is in window format, convert to frames\n",
    "    # If length is 510, it means 510 windows are there.\n",
    "    # So from the 1st to 509th window, take the first frame. For the 510th window, take all the frames. Total - 509 + 8 = 517 frames\n",
    "    for i in range(len(reconstructed_video) - 1):\n",
    "        output_video.append(reconstructed_video[i][0])  # First frame of each window\n",
    "    # Concatenate all the frames from the final window\n",
    "    output_video.extend(reconstructed_video[-1])\n",
    "    # Windowing code creates 1 window less than required. So duplicate last frame\n",
    "    output_video.append(output_video[-1])\n",
    "    output_video = np.array(output_video)\n",
    "\n",
    "    if background_subtraction:\n",
    "        # [-1,1] Normalisation. Only apply if background_subtraction is turned on.\n",
    "        output_video = 2.0 * (output_video - np.min(output_video)) / np.ptp(output_video) - 1\n",
    "\n",
    "        # Remove first 120 elements. This is because background subtraction history is 120.\n",
    "        # So those intial elements will be black\n",
    "        original_video = original_video[120:]\n",
    "        preprocessed_video = preprocessed_video[120:]\n",
    "        modified_video = modified_video[120:]\n",
    "        output_video = output_video[120:]\n",
    "        labels = labels[120:]\n",
    "\n",
    "    # print(len(original_video), len(preprocessed_video), len(modified_video), len(output_video))\n",
    "\n",
    "    for index in range(len(original_video)):\n",
    "\n",
    "        original_frame = cv2.resize(original_video[index], (display_ht, display_wd))\n",
    "        # uint8 to float32, scale down by 255\n",
    "        original_frame = (np.array(original_frame, dtype=np.float32)) / 255\n",
    "\n",
    "        preprocessed_frame = cv2.resize(preprocessed_video[index], (display_ht, display_wd))\n",
    "        # Only has height, width. So add a dimension for channel\n",
    "        preprocessed_frame = np.expand_dims(preprocessed_frame, axis=-1)\n",
    "        # float64 to float32\n",
    "        preprocessed_frame = np.array(preprocessed_frame, dtype=np.float32)\n",
    "        # Convert image from greyscale to RGB (To obtain 3 channels)\n",
    "        preprocessed_frame = cv2.cvtColor(preprocessed_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        modified_frame = cv2.resize(modified_video[index], (display_ht, display_wd))\n",
    "        # Only has height, width. So add a dimension for channel\n",
    "        modified_frame = np.expand_dims(modified_frame, axis=-1)\n",
    "        # uint8 to float32\n",
    "        modified_frame = np.array(modified_frame, dtype=np.float32)\n",
    "        # Convert image from greyscale to RGB (To obtain 3 channels)\n",
    "        modified_frame = cv2.cvtColor(modified_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        output_frame = cv2.resize(output_video[index], (display_ht, display_wd))\n",
    "        # Only has height, width. So add a dimension for channel\n",
    "        output_frame = np.expand_dims(output_frame, axis=-1)\n",
    "        # float32\n",
    "        # output_frame = np.array(output_frame, dtype=np.float32)\n",
    "        # Convert image from greyscale to RGB (To obtain 3 channels)\n",
    "        output_frame = cv2.cvtColor(output_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        horizontal_concatenation_1 = np.concatenate([original_frame, preprocessed_frame], axis=1)\n",
    "        horizontal_concatenation_2 = np.concatenate([modified_frame, output_frame], axis=1)\n",
    "        vertical_concatenation = np.concatenate([horizontal_concatenation_1, horizontal_concatenation_2], axis=0)\n",
    "\n",
    "        cv2.imshow(\"Original, Preprocessed, Modified, Output\", vertical_concatenation)\n",
    "\n",
    "        if labels[index] == 1:\n",
    "            print(\"Fall at Frame - {}\".format(index))\n",
    "\n",
    "        k = cv2.waitKey(ms_per_frame) & 0xFF\n",
    "        # Exit on 'esc' key\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def demo_pipeline_unimodality(name, dset, path, modelpath):\n",
    "    (\n",
    "        Test_Dataset,\n",
    "        test_dataloader,\n",
    "        x_data_test,\n",
    "        y_data_test,\n",
    "        x_info_test,\n",
    "    ) = create_pytorch_dataset(name, dset, path, window_len, fair_comparison, stride, TOD)\n",
    "\n",
    "    filepath_model = project_directory + \"\\Output\\Models\\Demo\\\\\" + modelpath\n",
    "\n",
    "    # Prepare GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load the model. Using Base_3DCAE\n",
    "    model = parameters.models[0]().to(device)\n",
    "    model.load_state_dict(torch.load(filepath_model))  # Load saved model weights\n",
    "    model.eval()  # Sets the model in testing mode.\n",
    "\n",
    "    print(\"Device Used - \" + device)\n",
    "    print(\"Key Frame Extraction - {}\".format(key_frame_extraction))\n",
    "    if key_frame_extraction:\n",
    "        print(\"Key Frame Extraction Algorithm - {}\".format(key_frame_extraction_algorithm))\n",
    "    print(\"Feature Extraction - {}\".format(feature_extraction))\n",
    "    if feature_extraction:\n",
    "        print(\"Background Subtraction - {}\".format(background_subtraction))\n",
    "        if background_subtraction:\n",
    "            print(\"Background Subtraction Algorithm - {}\".format(background_subtraction_algorithm))\n",
    "    print(\"Data Augmentation - {}\".format(data_augmentation))\n",
    "    print(\"Frame rate adjusted dataset - {}\".format(frame_rate_adjusted_dataset))\n",
    "    print(\"Window Length = {}\\n\".format(window_len))\n",
    "    print(\"{} Demo Test Videos - {}\\n\".format(dset, len(test_dataloader)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (sample, labels) in enumerate(test_dataloader):\n",
    "            vid_folder = x_info_test[i]\n",
    "            print((vid_folder))\n",
    "\n",
    "            # forward pass to get output\n",
    "            print(\"Forward Pass Initiated\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            sample = sample.to(device, dtype=torch.float)\n",
    "            chunks = torch.split(sample, forward_chunk_size, dim=1)\n",
    "            label_chunks = torch.split(labels, forward_chunk_size, dim=1)\n",
    "            recon_vid = []\n",
    "\n",
    "            for chunk, label_chunk in zip(chunks, label_chunks):\n",
    "                output = model(chunk)\n",
    "                output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                recon_vid.append(output)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            output = torch.cat(recon_vid, dim=1)\n",
    "\n",
    "            # convert tensors to numpy arrays for easy manipluations\n",
    "            sample = sample.data.cpu().numpy()\n",
    "            output = output.data.cpu().numpy()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "\n",
    "            print(\"Forward Pass Completed\")\n",
    "\n",
    "            # modified_video and original labels are not windowed. output is in windowed format\n",
    "            modified_video = x_data_test[i]\n",
    "            original_labels = y_data_test[i]\n",
    "\n",
    "            display_videos(name, dset, path, vid_folder, modified_video, output, original_labels)\n",
    "\n",
    "\n",
    "# Unimodality\n",
    "list_of_files = [\"Thermal\", \"ONI_IR\", \"IP\"]\n",
    "list_of_datasets = [\"Thermal_T3\", \"ONI_IR_T\", \"IP_T\"]\n",
    "\n",
    "# These are file paths for Base_3DCAE model only. Cannot be used for other Unimodal models\n",
    "if background_subtraction:\n",
    "    # Default GMG with L1loss\n",
    "    thermal_model = \"Thermal_T3_2024-03-13-08-07-43\"  # Trial-6\n",
    "    # Default GMG with L1loss()\n",
    "    oni_ir_model = \"ONI_IR_T_2024-03-15-04-14-00\"  # Trial-2\n",
    "    # Default GMG with MSEloss()\n",
    "    ip_model = \"IP_T_2024-03-15-23-52-18\"  # Trial-3\n",
    "else:\n",
    "    # Baseline Models\n",
    "    thermal_model = \"Thermal_T3_2024-03-11-21-35-40\"  # Trial-2\n",
    "    oni_ir_model = \"ONI_IR_T_2024-02-25-14-02-21\"  # Trial-1\n",
    "    ip_model = \"IP_T_2024-02-24-15-11-13\"  # Trial-1\n",
    "\n",
    "list_of_models = [thermal_model, oni_ir_model, ip_model]\n",
    "\n",
    "modality_index = 0  # 0 to 2\n",
    "\n",
    "dset = list_of_files[modality_index]\n",
    "name = list_of_datasets[modality_index]\n",
    "modelpath = list_of_models[modality_index]\n",
    "path = \"{}\\Dataset\\H5PY\\{}_Data_set-{}-imgdim64x64.h5\".format(project_directory, dataset_category, name)\n",
    "\n",
    "demo_pipeline_unimodality(name, dset, path, modelpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_base_paper_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
