{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import re\n",
    "from glob import glob\n",
    "from dataset_loader import create_multimodal_pytorch_dataset\n",
    "import parameters\n",
    "from functions import get_total_performance_metrics\n",
    "from functions import get_global_performance_metrics\n",
    "from functions import get_performance_metrics, get_multimodal_performance_metrics\n",
    "\n",
    "window_len = parameters.window_len\n",
    "stride = parameters.stride\n",
    "fair_comparison = parameters.fair_comparison\n",
    "device = parameters.device\n",
    "key_frame_extraction = parameters.key_frame_extraction\n",
    "key_frame_extraction_algorithm = parameters.key_frame_extraction_algorithm\n",
    "feature_extraction = parameters.feature_extraction\n",
    "background_subtraction = parameters.background_subtraction\n",
    "background_subtraction_algorithm = parameters.background_subtraction_algorithm\n",
    "data_augmentation = parameters.data_augmentation\n",
    "anomaly_detection_model = parameters.anomaly_detection_model\n",
    "dropout = parameters.dropout\n",
    "learning_rate = parameters.learning_rate\n",
    "num_epochs = parameters.num_epochs\n",
    "chunk_size = parameters.chunk_size\n",
    "forward_chunk_size = parameters.forward_chunk_size\n",
    "spatial_temporal_loss = parameters.spatial_temporal_loss\n",
    "frame_rate_adjusted_dataset = parameters.frame_rate_adjusted_dataset\n",
    "synchronise_video = parameters.synchronise_video\n",
    "pad_video = parameters.pad_video\n",
    "fusion_type = parameters.fusion_type\n",
    "\n",
    "dataset_category = parameters.dataset_category\n",
    "w1 = parameters.w1\n",
    "w2 = parameters.w2\n",
    "loss_fn = parameters.loss_fn\n",
    "TOD = parameters.TOD\n",
    "project_directory = parameters.project_directory\n",
    "dataset_directory = parameters.dataset_directory\n",
    "ht = parameters.ht\n",
    "wd = parameters.wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display Original Video, h5py file Video, Input Video and Reconstructed Output Video\n",
    "def display_videos(\n",
    "    names,\n",
    "    dsets,\n",
    "    paths,\n",
    "    vid_folder_list,\n",
    "    original_indices_list,\n",
    "    input_video_list,\n",
    "    reconstructed_video_list,\n",
    "    original_labels_list,\n",
    "):\n",
    "\n",
    "    display_ht = 256\n",
    "    display_wd = 256\n",
    "    # Video is adjusted to 8 fps. So each frame can be 125 ms\n",
    "    ms_per_frame = 50  # millisecond per frame\n",
    "\n",
    "    # Extract if fall folder or ADL folder. Can be taken from any modality\n",
    "    dir_type = re.findall(\"[a-zA-Z]+\", vid_folder_list[0])[0]\n",
    "\n",
    "    # Original Video\n",
    "    original_video_list = []\n",
    "    for i in range(2):\n",
    "        vid_location = \"{}\\Dataset\\Fall-Data\\{}\\{}\\{}\\{}\".format(\n",
    "            dataset_directory, dataset_category, dsets[i], dir_type, vid_folder_list[i]\n",
    "        )\n",
    "        vid_location = glob(vid_location + \"/*.jpg\") + glob(vid_location + \"/*.png\")\n",
    "        vid_location.sort(key=lambda var: [int(x) if x.isdigit() else x for x in re.findall(r\"[^0-9]|[0-9]+\", var)])\n",
    "        original_video = []\n",
    "        for filename in vid_location:\n",
    "            img = cv2.imread(filename, cv2.IMREAD_ANYCOLOR)\n",
    "            if img is not None:\n",
    "                original_video.append(img)\n",
    "        # Pick only the frames shortlisted in frame syncrhonisation\n",
    "        original_video = [original_video[j] for j in original_indices_list[i]]\n",
    "        original_video_list.append(original_video)\n",
    "\n",
    "    # Preprocessed Video\n",
    "    preprocessed_video_list = []\n",
    "    for i in range(2):\n",
    "        with h5py.File(paths[i], \"r\") as hf:\n",
    "            data_dict = hf[\"{}/Processed/Split_by_video\".format(names[i])]\n",
    "            preprocessed_video = data_dict[vid_folder_list[i]][\"Data\"][:]\n",
    "            # Pick only the frames shortlisted in frame syncrhonisation\n",
    "            preprocessed_video = [preprocessed_video[j] for j in original_indices_list[i]]\n",
    "            preprocessed_video_list.append(preprocessed_video)\n",
    "\n",
    "    # Modified Video\n",
    "    modified_video_list = []\n",
    "    for i in range(2):\n",
    "        input_video = input_video_list[i]\n",
    "        modified_video = []\n",
    "        # input_video is in windowed format, convert it to frames\n",
    "        for j in range(len(input_video) - 1):\n",
    "            modified_video.append(input_video[j][0])  # First frame of each window\n",
    "        # Concatenate all the frames from the final window\n",
    "        modified_video.extend(input_video[-1])\n",
    "        # Windowing code creates 1 window less than required. So duplicate last frame\n",
    "        modified_video.append(modified_video[-1])\n",
    "        modified_video = np.array(modified_video)\n",
    "        modified_video_list.append(modified_video)\n",
    "\n",
    "    # Output Video.\n",
    "    output_video_list = []\n",
    "    for i in range(2):\n",
    "        reconstructed_video = reconstructed_video_list[i]\n",
    "        output_video = []\n",
    "        # Reconstructed Video. It is in window format, convert to frames\n",
    "        # If length is 510, it means 510 windows are there.\n",
    "        # So from 0th to 509th, take the first frame. For the 510th window, take all the frames\n",
    "        for j in range(len(reconstructed_video) - 1):\n",
    "            output_video.append(reconstructed_video[j][0])  # First frame of each window\n",
    "        # Concatenate all the frames from the final window\n",
    "        output_video.extend(reconstructed_video[-1])\n",
    "        # Windowing code creates 1 window less than required. So duplicate last frame\n",
    "        output_video.append(output_video[-1])\n",
    "        output_video = np.array(output_video)\n",
    "        output_video_list.append(output_video)\n",
    "\n",
    "    # Labels\n",
    "    labels_list = []\n",
    "    for i in range(2):\n",
    "        original_label = original_labels_list[i]\n",
    "        labels = []\n",
    "        # original_label is in windowed labels format, convert it to frame level labels\n",
    "        for j in range(len(original_label) - 1):\n",
    "            labels.append(original_label[j][0])  # First label of each window (label of first frame)\n",
    "        # Concatenate all the frame labels from the final window\n",
    "        labels.extend(original_label[-1])\n",
    "        # Windowing code creates 1 window less than required. So duplicate last frame label\n",
    "        labels.append(labels[-1])\n",
    "        labels = np.array(labels)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    if background_subtraction:\n",
    "        for i in range(2):\n",
    "            # [-1,1] Normalisation. Only apply if background_subtraction is turned on.\n",
    "            output_video_list[i] = (\n",
    "                2.0 * (output_video_list[i] - np.min(output_video_list[i])) / np.ptp(output_video_list[i]) - 1\n",
    "            )\n",
    "\n",
    "            # Remove first 120 elements. This is because background subtraction history is 120.\n",
    "            # So those intial elements will be black\n",
    "            original_video_list[i] = original_video_list[i][120:]\n",
    "            preprocessed_video_list[i] = preprocessed_video_list[i][120:]\n",
    "            modified_video_list[i] = modified_video_list[i][120:]\n",
    "            output_video_list[i] = output_video_list[i][120:]\n",
    "            labels_list[i] = labels_list[i][120:]\n",
    "\n",
    "    # print(len(original_video_list[0]),len(preprocessed_video_list[0]),len(modified_video_list[0]),len(output_video_list[0]),len(labels_list[0]))  # fmt: skip\n",
    "    # print(len(original_video_list[1]),len(preprocessed_video_list[1]),len(modified_video_list[1]),len(output_video_list[1]),len(labels_list[1]))  # fmt: skip\n",
    "\n",
    "    for index in range(len(original_video_list[0])):\n",
    "\n",
    "        original_frame_list = []\n",
    "        for i in range(2):\n",
    "            original_frame = cv2.resize(original_video_list[i][index], (display_ht, display_wd))\n",
    "            # uint8 to float32\n",
    "            original_frame = (np.array(original_frame, dtype=np.float32)) / 255\n",
    "            original_frame_list.append(original_frame)\n",
    "\n",
    "        preprocessed_frame_list = []\n",
    "        for i in range(2):\n",
    "            preprocessed_frame = cv2.resize(preprocessed_video_list[i][index], (display_ht, display_wd))\n",
    "            preprocessed_frame = np.expand_dims(preprocessed_frame, axis=-1)\n",
    "            # float64 to float32\n",
    "            preprocessed_frame = np.array(preprocessed_frame, dtype=np.float32)\n",
    "            preprocessed_frame = cv2.cvtColor(preprocessed_frame, cv2.COLOR_GRAY2RGB)\n",
    "            preprocessed_frame_list.append(preprocessed_frame)\n",
    "\n",
    "        modified_frame_list = []\n",
    "        for i in range(2):\n",
    "            modified_frame = cv2.resize(modified_video_list[i][index], (display_ht, display_wd))\n",
    "            modified_frame = np.expand_dims(modified_frame, axis=-1)\n",
    "            # uint8 to float32\n",
    "            modified_frame = np.array(modified_frame, dtype=np.float32)\n",
    "            modified_frame = cv2.cvtColor(modified_frame, cv2.COLOR_GRAY2RGB)\n",
    "            modified_frame_list.append(modified_frame)\n",
    "\n",
    "        output_frame_list = []\n",
    "        for i in range(2):\n",
    "            output_frame = cv2.resize(output_video_list[i][index], (display_ht, display_wd))\n",
    "            output_frame = np.expand_dims(output_frame, axis=-1)\n",
    "            # float32\n",
    "            # output_frame = np.array(output_frame, dtype=np.float32)\n",
    "            output_frame = cv2.cvtColor(output_frame, cv2.COLOR_GRAY2RGB)\n",
    "            output_frame_list.append(output_frame)\n",
    "\n",
    "        modality_1_frames = np.concatenate(\n",
    "            [original_frame_list[0], preprocessed_frame_list[0], modified_frame_list[0], output_frame_list[0]], axis=1\n",
    "        )\n",
    "\n",
    "        # Rotate Images of Modality 2 by 90 Degree Clockwise\n",
    "        # Modified_frame and output_frame will already be in correct orientation\n",
    "\n",
    "        original_frame_list[1] = cv2.rotate(original_frame_list[1], cv2.ROTATE_90_CLOCKWISE)\n",
    "        preprocessed_frame_list[1] = cv2.rotate(preprocessed_frame_list[1], cv2.ROTATE_90_CLOCKWISE)\n",
    "        # If it is ONI_IR, Then Flip Horizontally\n",
    "        if dsets[1] == \"ONI_IR\":\n",
    "            original_frame_list[1] = cv2.flip(original_frame_list[1], 1)\n",
    "            preprocessed_frame_list[1] = cv2.flip(preprocessed_frame_list[1], 1)\n",
    "\n",
    "        modality_2_frames = np.concatenate(\n",
    "            [original_frame_list[1], preprocessed_frame_list[1], modified_frame_list[1], output_frame_list[1]], axis=1\n",
    "        )\n",
    "\n",
    "        vertical_concatenation = np.concatenate([modality_1_frames, modality_2_frames], axis=0)\n",
    "\n",
    "        cv2.imshow(\"Original, Preprocessed, Modified, Output\", vertical_concatenation)\n",
    "\n",
    "        if labels_list[0][index] == labels_list[1][index] == 1:\n",
    "            print(\"Fall at Frame - {}\".format(index))\n",
    "\n",
    "        k = cv2.waitKey(ms_per_frame) & 0xFF\n",
    "        # Exit on 'esc' key\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def demo_pipeline_multimodality(names, dsets, paths, modelpath):\n",
    "    (\n",
    "        Test_Dataset,\n",
    "        test_dataloader,\n",
    "        multi_x_data_test,\n",
    "        multi_y_data_test,\n",
    "        multi_x_info_test,\n",
    "    ) = create_multimodal_pytorch_dataset(names, dsets, paths, window_len, fair_comparison, stride)\n",
    "\n",
    "    filepath_model = project_directory + \"\\Output\\Models\\Demo\\\\\" + modelpath\n",
    "\n",
    "    # Prepare GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load the model. Using MultiModal_3DCAE\n",
    "    model = parameters.multi_modal_models[0]().to(device)\n",
    "    model.load_state_dict(torch.load(filepath_model))  # Load saved model weights\n",
    "    model.eval()  # Sets the model in testing mode.\n",
    "\n",
    "    print(\"Device Used - \" + device)\n",
    "    print(\"Key Frame Extraction - {}\".format(key_frame_extraction))\n",
    "    if key_frame_extraction:\n",
    "        print(\"Key Frame Extraction Algorithm - {}\".format(key_frame_extraction_algorithm))\n",
    "    print(\"Feature Extraction - {}\".format(feature_extraction))\n",
    "    if feature_extraction:\n",
    "        print(\"Background Subtraction - {}\".format(background_subtraction))\n",
    "        if background_subtraction:\n",
    "            print(\"Background Subtraction Algorithm - {}\".format(background_subtraction_algorithm))\n",
    "    print(\"Data Augmentation - {}\".format(data_augmentation))\n",
    "    print(\"Frame rate adjusted dataset - {}\".format(frame_rate_adjusted_dataset))\n",
    "    print(\"Synchronise Video - {}\".format(synchronise_video))\n",
    "    if synchronise_video:\n",
    "        print(\"Video length adjustment method - Not Applicable\")\n",
    "    else:\n",
    "        if pad_video:\n",
    "            print(\"Video length adjustment method - Pad Minimum\")\n",
    "        else:\n",
    "            print(\"Video length adjustment method - Trim Maximum\")\n",
    "    print(\"Window Length = {}\\n\".format(window_len))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (sample, labels, original_indices_list) in enumerate(test_dataloader):\n",
    "            # Get vid_folder across both modalities\n",
    "            vid_folder_list = np.array(multi_x_info_test)[:, i]\n",
    "            # Both folders will be same\n",
    "            print(\"{} - {}, {} - {}\".format(dsets[0], vid_folder_list[0], dsets[1], vid_folder_list[1]))\n",
    "\n",
    "            # forward pass to get output\n",
    "            print(\"Forward Pass Initiated\")\n",
    "\n",
    "            sample1 = sample[:, 0, :, :, :, :]\n",
    "            sample2 = sample[:, 1, :, :, :, :]\n",
    "\n",
    "            labels1 = labels[:, 0, :, :]\n",
    "            labels2 = labels[:, 1, :, :]\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            sample1 = sample1.to(device, dtype=torch.float)\n",
    "            sample2 = sample2.to(device, dtype=torch.float)\n",
    "\n",
    "            chunks1 = torch.split(sample1, chunk_size, dim=1)\n",
    "            chunks2 = torch.split(sample2, chunk_size, dim=1)\n",
    "\n",
    "            recon_vid1 = []\n",
    "            recon_vid2 = []\n",
    "\n",
    "            for chunk1, chunk2 in zip(chunks1, chunks2):\n",
    "                output1, output2 = model(chunk1, chunk2)\n",
    "                output1 = output1.to(device).permute(1, 0, 2, 3, 4)\n",
    "                output2 = output2.to(device).permute(1, 0, 2, 3, 4)\n",
    "                recon_vid1.append(output1)\n",
    "                recon_vid2.append(output2)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            output1 = torch.cat(recon_vid1, dim=1)\n",
    "            output2 = torch.cat(recon_vid2, dim=1)\n",
    "            output = torch.stack((output1, output2), dim=1)\n",
    "\n",
    "            # convert tensors to numpy arrays for easy manipluations\n",
    "            sample1 = sample1.data.cpu().numpy()\n",
    "            output1 = output1.data.cpu().numpy()\n",
    "            labels1 = labels1.data.cpu().numpy()\n",
    "\n",
    "            sample2 = sample2.data.cpu().numpy()\n",
    "            output2 = output2.data.cpu().numpy()\n",
    "            labels2 = labels2.data.cpu().numpy()\n",
    "\n",
    "            sample = sample.data.cpu().numpy()\n",
    "            output = output.data.cpu().numpy()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "\n",
    "            print(\"Forward Pass Completed\")\n",
    "\n",
    "            # Accesing Index 0 as batch_size = 1\n",
    "            # original_indices_list - By frame synchronisation, frames are equalised (and some frames are removed).\n",
    "            # This list contains the selected frames original indices in the original / h5py data (for both the modalities).\n",
    "            # Sample - Windowed Modified Input\n",
    "            # Output - Windowed Reconstructed Output\n",
    "            # Labels - Windowed Class Labels\n",
    "\n",
    "            display_videos(\n",
    "                names, dsets, paths, vid_folder_list, original_indices_list[0], sample[0], output[0], labels[0]\n",
    "            )\n",
    "\n",
    "\n",
    "# Multimodality\n",
    "list_of_files = [[\"Thermal\", \"ONI_IR\"], [\"Thermal\", \"IP\"]]\n",
    "list_of_datasets = [[\"Thermal_T3\", \"ONI_IR_T\"], [\"Thermal_T3\", \"IP_T\"]]\n",
    "\n",
    "# These are file paths for MultiModal_3DCAE model only. Cannot be used for other MultiModal models\n",
    "if background_subtraction:\n",
    "    # Both are FPS Adjusted, Default GMG, Smooth L1 Loss, Synchronise Video\n",
    "    thermal_oni_ir_model = \"MultiModal_Thermal_T3_ONI_IR_T_2024-04-17-14-58-26\"  # Trial-14\n",
    "    thermal_ip_model = \"MultiModal_Thermal_T3_IP_T_2024-04-18-09-19-42\"  # Trial-8\n",
    "else:\n",
    "    # Both are FPS Adjusted, L1 Loss, Synchronise Video\n",
    "    thermal_oni_ir_model = \"MultiModal_Thermal_T3_ONI_IR_T_2024-04-24-16-37-55\"  # Trial-7\n",
    "    thermal_ip_model = \"MultiModal_Thermal_T3_IP_T_2024-04-24-17-38-22\"  # Trial-4\n",
    "\n",
    "list_of_models = [thermal_oni_ir_model, thermal_ip_model]\n",
    "\n",
    "modalities_index = 0  # 0 or 1\n",
    "\n",
    "dsets = list_of_files[modalities_index]\n",
    "names = list_of_datasets[modalities_index]\n",
    "modelpath = list_of_models[modalities_index]\n",
    "paths = [f\"{project_directory}\\Dataset\\H5PY\\{dataset_category}_Data_set-{name}-imgdim64x64.h5\" for name in names]\n",
    "\n",
    "demo_pipeline_multimodality(names, dsets, paths, modelpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_base_paper_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
