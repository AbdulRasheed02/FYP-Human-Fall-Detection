{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import re\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "import parameters\n",
    "from dataset_loader import create_pytorch_dataset\n",
    "\n",
    "sys.path.remove(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "window_len = parameters.window_len\n",
    "stride = parameters.stride\n",
    "fair_comparison = parameters.fair_comparison\n",
    "TOD = parameters.TOD\n",
    "\n",
    "device = parameters.device\n",
    "key_frame_extraction = parameters.key_frame_extraction\n",
    "key_frame_extraction_algorithm = parameters.key_frame_extraction_algorithm\n",
    "feature_extraction = parameters.feature_extraction\n",
    "background_subtraction = parameters.background_subtraction\n",
    "background_subtraction_algorithm = parameters.background_subtraction_algorithm\n",
    "background_subtraction_algorithms = parameters.background_subtraction_algorithms\n",
    "data_augmentation = parameters.data_augmentation\n",
    "anomaly_detection_model = parameters.anomaly_detection_model\n",
    "\n",
    "frame_rate_adjusted_dataset = parameters.frame_rate_adjusted_dataset\n",
    "dataset_category = parameters.dataset_category\n",
    "project_directory = parameters.project_directory\n",
    "dataset_directory = parameters.dataset_directory\n",
    "ht = parameters.ht\n",
    "wd = parameters.wd\n",
    "\n",
    "display_ht = 450\n",
    "display_wd = 450\n",
    "# Video can be 4fps / 8fps / 20fps. So each frame can be displayed for 250 ms / 125 ms / 50 ms\n",
    "ms_per_frame = 125  # millisecond per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display Preprocessed Video and Input Video\n",
    "def display_videos(name, dset, path, vid_folder, input_video):\n",
    "\n",
    "    # Extract if fall folder or ADL folder\n",
    "    dir_type = re.findall(\"[a-zA-Z]+\", vid_folder)[0]\n",
    "\n",
    "    # Original Video\n",
    "    vid_location = \"{}\\Dataset\\Fall-Data\\{}\\{}\\{}\\{}\".format(\n",
    "        dataset_directory, dataset_category, dset, dir_type, vid_folder\n",
    "    )\n",
    "    vid_location = glob(vid_location + \"/*.jpg\") + glob(vid_location + \"/*.png\")\n",
    "    vid_location.sort(key=lambda var: [int(x) if x.isdigit() else x for x in re.findall(r\"[^0-9]|[0-9]+\", var)])\n",
    "    original_video = []\n",
    "    for filename in vid_location:\n",
    "        img = cv2.imread(filename, cv2.IMREAD_ANYCOLOR)\n",
    "        if img is not None:\n",
    "            original_video.append(img)\n",
    "\n",
    "    # Preprocessed Video\n",
    "    with h5py.File(path, \"r\") as hf:\n",
    "        data_dict = hf[\"{}/Processed/Split_by_video\".format(name)]\n",
    "        preprocessed_video = data_dict[vid_folder][\"Data\"][:]\n",
    "\n",
    "    # Input Video\n",
    "    input_video = input_video\n",
    "\n",
    "    # Length will not be equal. (Key Frame Extraction must be true)\n",
    "    # print(len(original_video), len(preprocessed_video), len(input_video))\n",
    "\n",
    "    print(\"Original Length -\", len(original_video))\n",
    "    print(\"Length after Key Frame Extraction -\", len(input_video))\n",
    "\n",
    "    # Display frames of the original and preprocessed video\n",
    "    for index in range(len(original_video)):\n",
    "        original_frame = cv2.resize(original_video[index], (display_ht, display_wd))\n",
    "        # uint8 to float32, scale down by 255\n",
    "        original_frame = (np.array(original_frame, dtype=np.float32)) / 255\n",
    "\n",
    "        preprocessed_frame = cv2.resize(preprocessed_video[index], (display_ht, display_wd))\n",
    "        # Only has height, width. So add a dimension for channel\n",
    "        preprocessed_frame = np.expand_dims(preprocessed_frame, axis=-1)\n",
    "        # float64 to float32\n",
    "        preprocessed_frame = np.array(preprocessed_frame, dtype=np.float32)\n",
    "        # Convert image from greyscale to RGB (To obtain 3 channels)\n",
    "        preprocessed_frame = cv2.cvtColor(preprocessed_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        horizontal_concatenation = np.concatenate([original_frame, preprocessed_frame], axis=1)\n",
    "\n",
    "        cv2.imshow(\"Original, Preprocessed\", horizontal_concatenation)\n",
    "        k = cv2.waitKey(ms_per_frame) & 0xFF\n",
    "        # Exit on 'esc' key\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(3000)  # Wait 3 seconds\n",
    "\n",
    "    # Display frames of the input video (Key Frame Extracted)\n",
    "    for index in range(len(input_video)):\n",
    "        input_frame = cv2.resize(input_video[index], (display_ht, display_wd))\n",
    "        # Only has height, width. So add a dimension for channel\n",
    "        input_frame = np.expand_dims(input_frame, axis=-1)\n",
    "        # uint8 to float32\n",
    "        input_frame = np.array(input_frame, dtype=np.float32)\n",
    "        # Convert image from greyscale to RGB (To obtain 3 channels)\n",
    "        input_frame = cv2.cvtColor(input_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        cv2.imshow(\"Key Frame - Input\", input_frame)\n",
    "        k = cv2.waitKey(ms_per_frame) & 0xFF\n",
    "        # Exit on 'esc' key\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def demo_pipeline_key_frame_extraction(name, dset, path):\n",
    "    (\n",
    "        Test_Dataset,\n",
    "        test_dataloader,\n",
    "        Train_Dataset,\n",
    "        train_dataloader,\n",
    "        x_data_test,\n",
    "        y_data_test,\n",
    "        x_info_test,\n",
    "        x_data_train,\n",
    "        y_data_train,\n",
    "        x_info_train,\n",
    "    ) = create_pytorch_dataset(name, dset, path, window_len, fair_comparison, stride, TOD)\n",
    "\n",
    "    print(\"Key Frame Extraction - {}\".format(key_frame_extraction))\n",
    "    if key_frame_extraction:\n",
    "        print(\"Key Frame Extraction Algorithm - {}\".format(key_frame_extraction_algorithm))\n",
    "    print(\"Feature Extraction - {}\".format(feature_extraction))\n",
    "    if feature_extraction:\n",
    "        print(\"Background Subtraction - {}\".format(background_subtraction))\n",
    "        if background_subtraction:\n",
    "            print(\"Background Subtraction Algorithm - {}\".format(background_subtraction_algorithm))\n",
    "    print(\"Frame rate adjusted dataset - {}\".format(frame_rate_adjusted_dataset))\n",
    "    print(\"{} Demo Train Videos - {}\".format(dset, len(train_dataloader)))\n",
    "    print(\"{} Demo Test Videos - {}\".format(dset, len(test_dataloader)))\n",
    "\n",
    "    for loader_index, dataloader in enumerate([test_dataloader, train_dataloader]):\n",
    "        for i, (sample, labels) in enumerate(dataloader):\n",
    "\n",
    "            # sample and labels are in windowed format\n",
    "\n",
    "            # Test Dataloader\n",
    "            if loader_index == 0:\n",
    "                vid_folder = x_info_test[i]\n",
    "                input_video = x_data_test[i]  # input_video is not windowed.\n",
    "            # Train Dataloader\n",
    "            else:\n",
    "                vid_folder = x_info_train[i]\n",
    "                input_video = x_data_train[i]  # input_video is not windowed.\n",
    "\n",
    "            print(\"\")\n",
    "            print((vid_folder))\n",
    "\n",
    "            # Input Video - Input\n",
    "            display_videos(name, dset, path, vid_folder, input_video)\n",
    "\n",
    "\n",
    "# Unimodality\n",
    "list_of_files = [\"Thermal\", \"ONI_IR\", \"IP\"]\n",
    "list_of_datasets = [\"Thermal_T3\", \"ONI_IR_T\", \"IP_T\"]\n",
    "\n",
    "modality_index = 0  # 0 to 2\n",
    "\n",
    "dset = list_of_files[modality_index]\n",
    "name = list_of_datasets[modality_index]\n",
    "path = \"{}\\Dataset\\H5PY\\{}_Data_set-{}-imgdim64x64.h5\".format(project_directory, dataset_category, name)\n",
    "\n",
    "demo_pipeline_key_frame_extraction(name, dset, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_base_paper_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
