{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import ffmpeg\n",
    "import pdb\n",
    "import parameters\n",
    "from dataset_loader import create_multimodal_pytorch_dataset\n",
    "from functions import get_total_performance_metrics\n",
    "from functions import get_performance_metrics\n",
    "from functions import get_global_performance_metrics\n",
    "from functions import get_window_metrics\n",
    "from functions import get_frame_metrics\n",
    "from functions import animate\n",
    "from functions import get_multimodal_performance_metrics\n",
    "from functions import late_fusion_performance_metricsV4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "window_len = parameters.window_len\n",
    "stride = parameters.stride\n",
    "fair_comparison = parameters.fair_comparison\n",
    "\n",
    "device = parameters.device\n",
    "\n",
    "dropout = parameters.dropout\n",
    "learning_rate = parameters.learning_rate\n",
    "num_epochs = parameters.num_epochs\n",
    "chunk_size = parameters.chunk_size\n",
    "forward_chunk = parameters.forward_chunk\n",
    "forward_chunk_size = parameters.forward_chunk_size\n",
    "spatial_temporal_loss = parameters.spatial_temporal_loss\n",
    "pad_video = parameters.pad_video\n",
    "frame_rate_adjusted_dataset = parameters.frame_rate_adjusted_dataset\n",
    "w1 = parameters.w1\n",
    "w2 = parameters.w2\n",
    "loss_fn = parameters.loss_fn\n",
    "TOD = parameters.TOD\n",
    "\n",
    "project_directory = parameters.project_directory\n",
    "\n",
    "\n",
    "def full_pipeline(names, dsets, paths, modelpath):\n",
    "\n",
    "    # Lets load the H5PY dataset into a pytorch dataset class.Please see dataset_creator on how to generate the H5PY file.\n",
    "    Test_Dataset, test_dataloader, Train_Dataset, train_dataloader = create_multimodal_pytorch_dataset(\n",
    "        names, dsets, paths, window_len, fair_comparison, stride\n",
    "    )\n",
    "    print(\"Train Dataloader - {}\".format(len(train_dataloader)))\n",
    "    print(\"Test Dataloader - {}\\n\".format(len(test_dataloader)))\n",
    "\n",
    "    # Prepare for GPU training\n",
    "    print(\"Device Used - \" + device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Select which model to use\n",
    "    model = parameters.multi_modal_model().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Printing methodology\n",
    "    # Printing the class name of the model being used\n",
    "    print(\"\\nModel Used - \" + model.__class__.__name__)\n",
    "    print(\"Feature Extraction - {}\".format(parameters.feature_extraction))\n",
    "    if parameters.feature_extraction:\n",
    "        print(\"Background Subtraction - {}\".format(parameters.background_subtraction))\n",
    "        if parameters.background_subtraction:\n",
    "            print(\"Background Subtraction Algorithm - {}\".format(parameters.background_subtraction_algorithm))\n",
    "    print(\"Data Augmentation - {}\".format(parameters.data_augmentation))\n",
    "    print(\"Spatial Temporal Loss - {}\".format(spatial_temporal_loss))\n",
    "    if spatial_temporal_loss:\n",
    "        print(\"w1 - {}, w2 - {}\".format(w1, w2))\n",
    "    print(\"\\nFrame rate adjusted dataset - {}\".format(frame_rate_adjusted_dataset))\n",
    "    if pad_video:\n",
    "        print(\"Video length adjustment method - Pad Minimum\")\n",
    "    else:\n",
    "        print(\"Video length adjustment method - Trim Maximum\")\n",
    "\n",
    "    # Printing Parameters\n",
    "    print(\n",
    "        \"\\nWindow Length = {}\\nStride = {}\\nFair Comparison = {}\\nDropout = {}\\nLearning Rate = {}\\nNum Epochs = {}\\nChunk Size = {}\\nForward Chunk = {}\\nForward Chunk Size = {}\\nLoss Fn = {}\\n\".format(\n",
    "            window_len,\n",
    "            stride,\n",
    "            fair_comparison,\n",
    "            dropout,\n",
    "            learning_rate,\n",
    "            num_epochs,\n",
    "            chunk_size,\n",
    "            forward_chunk,\n",
    "            forward_chunk_size,\n",
    "            loss_fn,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Training our model\n",
    "    def train_model(filepath):\n",
    "        print(\"Training has Begun\")\n",
    "        model.train()  # Sets the model in training mode.\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            val_loss = 0  # Not used\n",
    "\n",
    "            for i, (sample, labels) in enumerate(train_dataloader):\n",
    "                # sample shape - (batch_size, modalities, # of windows w/in video, window-length, 64, 64). Ex - torch.Size([1, 2, 819, 8, 64, 64])\n",
    "                # labels shape - (batch_size, modalities, # of windows w/in video, window-length). Ex - torch.Size([1, 2, 819, 8])\n",
    "\n",
    "                # Extract sample of first modality\n",
    "                # sample1 shape - (batch_size, # of windows w/in video, window-length, 64, 64). Ex - torch.Size([1, 819, 8, 64, 64])\n",
    "                sample1 = sample[:, 0, :, :, :, :]\n",
    "                # Extract sample of second modality\n",
    "                sample2 = sample[:, 1, :, :, :, :]\n",
    "                # print(sample1.shape, sample2.shape)\n",
    "\n",
    "                # Extract label of first modality\n",
    "                # labels_1 shape - (batch_size, # of windows w/in video, window-length). Ex - torch.Size([1, 814, 8])\n",
    "                labels1 = labels[:, 0, :, :]  # Not used\n",
    "                # Extract label of second modality\n",
    "                labels2 = labels[:, 1, :, :]  # Not used\n",
    "                # print(labels1.shape, labels2.shape)\n",
    "\n",
    "                # Moves the input sample1, sample2 tensor to the specified device\n",
    "                sample1 = sample1.to(device, dtype=torch.float)\n",
    "                sample2 = sample2.to(device, dtype=torch.float)\n",
    "\n",
    "                # Splits the input sample1, sample2 into smaller chunks\n",
    "                chunks1 = torch.split(sample1, chunk_size, dim=1)\n",
    "                chunks2 = torch.split(sample2, chunk_size, dim=1)\n",
    "                # print(len(chunks1), len(chunks2))\n",
    "\n",
    "                for chunk1, chunk2 in zip(chunks1, chunks2):\n",
    "                    # ===================forward=====================\n",
    "                    # Perform a forward pass of the model on the current chunks\n",
    "                    output1, output2 = model(chunk1, chunk2)\n",
    "                    # Moves the output tensor to the device and permutes its dimensions.\n",
    "                    output1 = output1.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    output2 = output2.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    model.zero_grad()  # Clears the gradients of the model parameters.\n",
    "                    # Computes the loss between the reconstructed output and the original chunk of data.\n",
    "                    loss1 = loss_fn(output1, chunk1)\n",
    "                    loss2 = loss_fn(output2, chunk2)\n",
    "                    loss = loss1 + loss2\n",
    "                    # ===================backward====================\n",
    "                    loss.backward()  # Getting gradients of the loss w.r.t. model parameters\n",
    "                    optimizer.step()  # Updating model parameters\n",
    "                    optimizer.zero_grad()  # Clear gradients of the model parameters for next iteration\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            # ===================log========================\n",
    "            print(\"epoch [{}/{}], loss:{:.4f}\".format(epoch + 1, num_epochs, loss.item()))\n",
    "            torch.save(model.state_dict(), filepath)  # save the model each epoch at location filepath\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Training has Completed\\n\")\n",
    "\n",
    "    # Forward pass of model on test dataset\n",
    "    def forward_pass(path):\n",
    "        model.load_state_dict(torch.load(path))  # Load saved model weights\n",
    "        model.eval()  # Sets the model in testing mode.\n",
    "\n",
    "        frame_stats = []\n",
    "        window_stats = []\n",
    "\n",
    "        frame_stats1 = []\n",
    "        window_stats1 = []\n",
    "        frame_stats2 = []\n",
    "        window_stats2 = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            print(\"Forward pass occuring\")\n",
    "\n",
    "            for i, (sample, labels) in enumerate(test_dataloader):\n",
    "                # forward pass to get output\n",
    "                sample1 = sample[:, 0, :, :, :, :]\n",
    "                sample2 = sample[:, 1, :, :, :, :]\n",
    "\n",
    "                labels1 = labels[:, 0, :, :]\n",
    "                labels2 = labels[:, 1, :, :]\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                sample1 = sample1.to(device, dtype=torch.float)\n",
    "                sample2 = sample2.to(device, dtype=torch.float)\n",
    "\n",
    "                chunks1 = torch.split(sample1, chunk_size, dim=1)\n",
    "                chunks2 = torch.split(sample2, chunk_size, dim=1)\n",
    "\n",
    "                recon_vid1 = []\n",
    "                recon_vid2 = []\n",
    "                for chunk1, chunk2 in zip(chunks1, chunks2):\n",
    "                    output1, output2 = model(chunk1, chunk2)\n",
    "                    output1 = output1.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    output2 = output2.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    recon_vid1.append(output1)\n",
    "                    recon_vid2.append(output2)\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                output1 = torch.cat(recon_vid1, dim=1)\n",
    "                output2 = torch.cat(recon_vid2, dim=1)\n",
    "                output = torch.stack((output1, output2), dim=1)\n",
    "\n",
    "                # convert tensors to numpy arrays for easy manipluations\n",
    "                sample1 = sample1.data.cpu().numpy()\n",
    "                output1 = output1.data.cpu().numpy()\n",
    "                labels1 = labels1.data.cpu().numpy()\n",
    "\n",
    "                # Metrics for Output 1\n",
    "                frame_std, frame_mean, frame_labels, window_std, window_mean, window_labels = get_performance_metrics(\n",
    "                    sample1, output1, labels1, window_len\n",
    "                )\n",
    "                frame_stats1.append([frame_mean, frame_std, frame_labels])\n",
    "                window_stats1.append([window_mean, window_std, window_labels])\n",
    "\n",
    "                sample2 = sample2.data.cpu().numpy()\n",
    "                output2 = output2.data.cpu().numpy()\n",
    "                labels2 = labels2.data.cpu().numpy()\n",
    "\n",
    "                # Metrics for Output 2\n",
    "                frame_std, frame_mean, frame_labels, window_std, window_mean, window_labels = get_performance_metrics(\n",
    "                    sample2, output2, labels2, window_len\n",
    "                )\n",
    "                frame_stats2.append([frame_mean, frame_std, frame_labels])\n",
    "                window_stats2.append([window_mean, window_std, window_labels])\n",
    "\n",
    "                sample = sample.data.cpu().numpy()\n",
    "                output = output.data.cpu().numpy()\n",
    "                labels = labels.data.cpu().numpy()\n",
    "\n",
    "                # Metrics for Combined Output\n",
    "                # index 0 because batch_size=1, so accessing the first and only element.\n",
    "                frame_std, frame_mean, frame_labels, window_std, window_mean, window_labels = (\n",
    "                    get_multimodal_performance_metrics(sample[0], output[0], labels[0], window_len)\n",
    "                )\n",
    "                frame_stats.append([frame_mean, frame_std, frame_labels])\n",
    "                window_stats.append([window_mean, window_std, window_labels])\n",
    "\n",
    "                \"\"\"\n",
    "                if j % 50 == 0:\n",
    "                    animate(sample[0, :, :, :, :], output[0, :, :, :, :], frame_mean, dset, start_time)\n",
    "                \"\"\"\n",
    "\n",
    "            print(\"Forward pass completed\\n\")\n",
    "\n",
    "        return (frame_stats1, window_stats1, frame_stats2, window_stats2, frame_stats, window_stats)\n",
    "\n",
    "    start_time = str(datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "    modality = \"MultiModal_\" + names[0] + \"_\" + names[1] + \"_\" + start_time\n",
    "    filepath = project_directory + \"\\Output\\Models\\\\\" + modality\n",
    "\n",
    "    # Comment out this call if you dont want to train a model\n",
    "    train_model(filepath)\n",
    "\n",
    "    # Insert any modelpath instead of filepath to use a specified pre trained model\n",
    "    frame_stats1, window_stats1, frame_stats2, window_stats2, frame_stats, window_stats = forward_pass(filepath)\n",
    "\n",
    "    print(\"{}\\n\".format(modality))\n",
    "\n",
    "    # Metrics for Output 1\n",
    "    # get_total_performance_metrics(modality, frame_stats1, window_stats1, window_len)\n",
    "    # Metrics for Output 2\n",
    "    # get_total_performance_metrics(modality, frame_stats2, window_stats2, window_len)\n",
    "    # Metrics for Combined Output\n",
    "    get_total_performance_metrics(modality, frame_stats, window_stats, window_len)\n",
    "\n",
    "    # mean_AUROC, mean_AUPR, std_AUROC, std_AUPR = late_fusion_performance_metricsV4()\n",
    "\n",
    "    return ()\n",
    "\n",
    "\n",
    "# Directory names of the raw dataset from the Fall-Data folder\n",
    "# list_of_files = ['Thermal','ONI_IR','IP']\n",
    "list_of_files = [\"Thermal\", \"ONI_IR\"]\n",
    "\n",
    "# Dataset names used during H5PY file creation (dsets variable from dataset_creator.py)\n",
    "# list_of_datasets = ['Thermal_T3','ONI_IR_T','IP_T']\n",
    "list_of_datasets = [\"Thermal_T3\", \"ONI_IR_T\"]\n",
    "\n",
    "# Pre-trained model weight location if wanting to test pre-trained model. 'x' should be replaced with path to model weight location\n",
    "modelpath = \"x\"\n",
    "\n",
    "dsets = list_of_files\n",
    "names = list_of_datasets\n",
    "paths = [f\"{project_directory}\\Dataset\\H5PY\\Data_set-{name}-imgdim64x64.h5\" for name in names]\n",
    "full_pipeline(names, dsets, paths, modelpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_base_paper_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
