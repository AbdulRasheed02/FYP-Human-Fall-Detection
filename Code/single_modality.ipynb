{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook will allow you to train or run a model on an individual modality\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import os\n",
    "import ffmpeg\n",
    "import pdb\n",
    "import parameters\n",
    "from functions import create_pytorch_dataset\n",
    "from functions import get_total_performance_metrics\n",
    "from functions import get_performance_metrics\n",
    "from functions import get_global_performance_metrics\n",
    "from functions import get_window_metrics\n",
    "from functions import get_frame_metrics\n",
    "from functions import animate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lets load the H%PY dataset into a pytorch dataset class.Please see \n",
    "# dataset_creator on how to generate the H5PY file. \n",
    "\n",
    "window_len = parameters.window_len\n",
    "stride = parameters.stride\n",
    "fair_comparison = parameters.fair_comparison\n",
    "\n",
    "device = parameters.device\n",
    "\n",
    "dropout = parameters.dropout\n",
    "learning_rate = parameters.learning_rate\n",
    "num_epochs = parameters.num_epochs\n",
    "chunk_size = parameters.chunk_size\n",
    "forward_chunk = parameters.forward_chunk\n",
    "forward_chunk_size = parameters.forward_chunk_size\n",
    "loss_fn = parameters.loss_fn\n",
    "\n",
    "def full_pipeline(name, dset, window_len, fair_comparison, path, stride, modelpath):   \n",
    "\n",
    "    Test_Dataset, test_dataloader, Train_Dataset, train_dataloader = create_pytorch_dataset(name, dset, path, window_len, fair_comparison, stride, TOD = 'Both')\n",
    "    print('Train Dataloader - {}'.format(len(train_dataloader)))\n",
    "    print('Test Dataloader - {}'.format(len(test_dataloader)))\n",
    "    \n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Autoencoder, self).__init__()\n",
    "            # first layer\n",
    "            self.ec1 = nn.Conv3d(1, 32, (5, 3, 3), stride=1, padding=(2, 1, 1),)\n",
    "            self.em1 = nn.MaxPool3d((1, 2, 2), return_indices=True)\n",
    "            # second layer\n",
    "            self.ec2 = nn.Conv3d(32, 16, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.em2 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "            # third layer\n",
    "            self.ec3 = nn.Conv3d(16, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.em3 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "            # encoding done, time to decode\n",
    "            self.dc1 = nn.ConvTranspose3d(8, 16, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.dm1 = nn.MaxUnpool3d((2, 2, 2))\n",
    "            # inverse of 2nd Conv\n",
    "            self.dc2 = nn.ConvTranspose3d(16, 32, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.dm2 = nn.MaxUnpool3d((2, 2, 2))\n",
    "            # final inverse\n",
    "            self.dm3 = nn.MaxUnpool3d((1, 2, 2))\n",
    "            self.dc3 = nn.ConvTranspose3d(32, 1, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "\n",
    "        def forward(self, x):\n",
    "            # *** start of encoder\n",
    "            x = x.permute(1, 0, 2, 3, 4)  # reorder to have correct dimensions\n",
    "            # (batch_size, chanels, depth, width, height)\n",
    "            _ec1 = F.relu(self.ec1(x))\n",
    "            _em1, i1 = self.em1(_ec1)\n",
    "            #_ec1 = self.ed1(_ec1)\n",
    "            # second layer\n",
    "            _ec2 = F.relu(self.ec2(_em1))\n",
    "            _em2, i2 = self.em2(_ec2)\n",
    "            #_em2 = self.ed2(_em2)\n",
    "            # third layer\n",
    "            _ec3 = F.relu(self.ec3(_em2))\n",
    "            _em3, i3 = self.em3(_ec3)\n",
    "            # print(\"====== Encoding Done =========\")\n",
    "            _dm1 = self.dm1(_em3, i3, output_size=i2.size())\n",
    "            _dc1 = F.relu(self.dc1(_dm1))\n",
    "            _dm2 = self.dm2(_dc1, i2)\n",
    "            _dc2 = F.relu(self.dc2(_dm2))\n",
    "            _dm3 = self.dm3(_dc2, i1)\n",
    "            re_x = torch.tanh(self.dc3(_dm3))\n",
    "            return re_x\n",
    "\n",
    "    # Now lets train our model\n",
    "\n",
    "    # prepare for GPU training \n",
    "    print('Device Used - ' + device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # select which model - you could load your own or put it in the function above \n",
    "    model = Autoencoder().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train_model(filepath):\n",
    "        print(\"Training has Begun\")\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            val_loss = 0\n",
    "            frame_stats = [] \n",
    "            window_stats = [] \n",
    "            for i, (sample, labels) in enumerate(train_dataloader):\n",
    "                # ===================forward=====================\n",
    "                sample = sample.to(device, dtype=torch.float)\n",
    "                # split sample into smaller sizes due to GPU memory constraints\n",
    "                chunks = torch.split(sample, chunk_size, dim=1)\n",
    "                recon_vid = []\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    output = model(chunk)\n",
    "                    output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    model.zero_grad()\n",
    "                    loss = loss_fn(output, chunk)\n",
    "                    recon_vid.append(output)\n",
    "                    # ===================backward====================\n",
    "                    # Getting gradients w.r.t. parameters\n",
    "                    loss.backward()\n",
    "                    # Updating parameters\n",
    "                    optimizer.step()\n",
    "                    # Clear gradients w.r.t. parameters\n",
    "                    optimizer.zero_grad()\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                if epoch == num_epochs-1:\n",
    "                    output = torch.cat(recon_vid, dim=1)\n",
    "                    # convert tensors to numpy arrays for easy manipluations\n",
    "                    sample = sample.data.cpu().numpy()\n",
    "                    output = output.data.cpu().numpy()\n",
    "                    labels = labels.data.cpu().numpy()\n",
    "                    frame_std, frame_mean, frame_labels, window_std, window_mean, window_labels = get_performance_metrics(sample, output, labels, window_len)\n",
    "                    frame_stats.append([frame_mean, frame_std, frame_labels])\n",
    "                    window_stats.append([window_mean, window_std, window_labels])\n",
    "            \n",
    "            if epoch == num_epochs-1:\n",
    "                #get_total_performance_metrics(frame_stats, window_stats, window_len)\n",
    "                recon_errors = []\n",
    "                recon_labels = []\n",
    "                for i in range(len(frame_stats)):\n",
    "                    # print(i)\n",
    "                    # this a single video metrics\n",
    "                    frame_mean, frame_std, frame_labels = frame_stats[i]\n",
    "                    recon_errors.append([frame_mean, frame_std])\n",
    "                    recon_labels.append(frame_labels)    \n",
    "                np.save(project_directory+\"\\Output\\Recon_Errors\\\\train_recon_errors_{}.npy\".format(modality), recon_errors)\n",
    "                np.save(project_directory+\"\\Output\\Recon_Errors\\\\train_recon_labels_{}.npy\".format(modality), recon_labels)\n",
    "    \n",
    "            # ===================log========================\n",
    "            print(\"epoch [{}/{}], loss:{:.4f}\".format(epoch + 1, num_epochs, loss.item()))\n",
    "            torch.save(model.state_dict(), filepath) # save the model each epoch at location filepath\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Training has Completed\")\n",
    "        \n",
    "   \n",
    "    def foward_pass(path):\n",
    "        model.load_state_dict(torch.load(path)) # load a saved model \n",
    "        model.eval()\n",
    "\n",
    "        frame_stats = []\n",
    "        window_stats = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"foward pass occuring\")\n",
    "            # just forward pass of model on test dataset\n",
    "            for j, (sample, labels) in enumerate(test_dataloader):\n",
    "                # foward pass to get output\n",
    "                torch.cuda.empty_cache()\n",
    "                sample = sample.to(device, dtype=torch.float)\n",
    "                chunks = torch.split(sample, forward_chunk, dim=1)\n",
    "                recon_vid = []\n",
    "                for chunk in chunks:\n",
    "                    output = model(chunk)\n",
    "                    output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    recon_vid.append(output)\n",
    "                    torch.cuda.empty_cache()\n",
    "                output = torch.cat(recon_vid, dim=1)\n",
    "                # convert tensors to numpy arrays for easy manipluations\n",
    "                sample = sample.data.cpu().numpy()\n",
    "                output = output.data.cpu().numpy()\n",
    "                labels = labels.data.cpu().numpy()\n",
    "\n",
    "\n",
    "                frame_std, frame_mean, frame_labels, window_std, window_mean, window_labels = get_performance_metrics(sample, output, labels, window_len)\n",
    "                frame_stats.append([frame_mean, frame_std, frame_labels])\n",
    "                window_stats.append([window_mean, window_std, window_labels])\n",
    "                \n",
    "                #if j % 10 == 0:\n",
    "                    #print(sample.shape)\n",
    "                    #animate(sample[0, :, :, :, :], output[0, :, :, :, :], frame_mean, dset, start_time)\n",
    "                \n",
    "        return(frame_stats, window_stats)\n",
    "    \n",
    "    start_time = str(datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "    modality = (name + start_time)\n",
    "    filepath = (project_directory+\"\\Output\\Models\\\\\"+ modality + start_time)\n",
    "    # comment out this call if you dont want to train a model\n",
    "    train_model(filepath)\n",
    "\n",
    "    # INSERT modelpath instead of filepath to use a specified pre trained model \n",
    "    frame_stats, window_stats = foward_pass(filepath)\n",
    "    \n",
    "    print(modality)\n",
    "    get_total_performance_metrics(modality, frame_stats, window_stats, window_len)\n",
    "    get_global_performance_metrics(modality, frame_stats, window_stats, window_len)\n",
    "    \n",
    "    return() \n",
    "\n",
    "\n",
    "# Datasets name used during file H5PY file creation\n",
    "# list_of_datasets = ['ZED_RGB_T2','ZED_Depth_T2','ONI_Depth_T2','ONI_IR_T2', 'Thermal_T2', 'IP_T']\n",
    "list_of_datasets = ['Thermal_T3'] \n",
    "\n",
    "# Directory of where files are stored for the camera\n",
    "# list_of_files = ['TurncatedV1/ZED_RGB', 'TurncatedV1/ZED_Depth', 'TurncatedV1/ONI_Depth', 'TurncatedV1/ONI_IR', 'TurncatedV1/Thermal', 'TurncatedV1/IP', ] # 'EditsV1/IP',,\"EditsV1/ONI_Depth\", \"EditsV1/ZED_Depth\", \"EditsV1/ZED_RGB\", \"EditsV1/Thermal\",   \"EditsV1/ONI_IR\"\"EditsV1/IP\",\n",
    "list_of_files = ['Thermal']\n",
    "\n",
    "# List of pre-trained model weight location if wanting to test trained model \n",
    "# list_of_models = ['x','x','x','x','x','x'] # after training - it will save them in the Models folder\n",
    "list_of_models = ['x'] # after training - it will save them in the Models folder\n",
    "\n",
    "script_directory=os.getcwd()\n",
    "project_directory=os.path.dirname(script_directory)\n",
    "\n",
    "for i in range(len(list_of_datasets)):\n",
    "    modelpath = list_of_models[i]\n",
    "    name = list_of_datasets[i]\n",
    "    dset = list_of_files[i]\n",
    "    path = \"{}\\Dataset\\H5PY\\Data_set-{}-imgdim64x64.h5\".format(project_directory,name) \n",
    "    full_pipeline(name, dset, window_len, fair_comparison, path, stride, modelpath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
