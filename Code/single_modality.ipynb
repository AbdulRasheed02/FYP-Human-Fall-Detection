{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook will allow you to train or run a model on an individual modality\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from Models.Base_3DCAE import Base_3DCAE\n",
    "from io import StringIO\n",
    "import os\n",
    "import ffmpeg\n",
    "import pdb\n",
    "import parameters\n",
    "from dataset_loader import create_pytorch_dataset\n",
    "from functions import get_total_performance_metrics\n",
    "from functions import get_performance_metrics\n",
    "from functions import get_global_performance_metrics\n",
    "from functions import get_window_metrics\n",
    "from functions import get_frame_metrics\n",
    "from functions import animate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "window_len = parameters.window_len\n",
    "stride = parameters.stride\n",
    "fair_comparison = parameters.fair_comparison\n",
    "\n",
    "device = parameters.device\n",
    "\n",
    "dropout = parameters.dropout\n",
    "learning_rate = parameters.learning_rate\n",
    "num_epochs = parameters.num_epochs\n",
    "chunk_size = parameters.chunk_size\n",
    "forward_chunk = parameters.forward_chunk\n",
    "forward_chunk_size = parameters.forward_chunk_size\n",
    "loss_fn = parameters.loss_fn\n",
    "TOD = parameters.TOD\n",
    "\n",
    "project_directory = parameters.project_directory\n",
    "\n",
    "\n",
    "def full_pipeline(name, dset, window_len, fair_comparison, path, stride, modelpath):\n",
    "\n",
    "    # Lets load the H5PY dataset into a pytorch dataset class.Please see dataset_creator on how to generate the H5PY file.\n",
    "    Test_Dataset, test_dataloader, Train_Dataset, train_dataloader = create_pytorch_dataset(\n",
    "        name, dset, path, window_len, fair_comparison, stride, TOD\n",
    "    )\n",
    "    print(\"Train Dataloader - {}\".format(len(train_dataloader)))\n",
    "    print(\"Test Dataloader - {}\\n\".format(len(test_dataloader)))\n",
    "\n",
    "    # Prepare for GPU training\n",
    "    print(\"Device Used - \" + device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Select which model to use\n",
    "    model = Base_3DCAE().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Printing methodology\n",
    "    # Printing the class name of the model being used\n",
    "    print(\"\\nModel Used - \" + model.__class__.__name__)\n",
    "    print(\"Feature Extraction - {}\".format(parameters.feature_extraction))\n",
    "    if parameters.feature_extraction:\n",
    "        print(\"Background Subtraction - {}\".format(parameters.background_subtraction))\n",
    "        if parameters.background_subtraction:\n",
    "            print(\"Background Subtraction Algorithm - {}\".format(parameters.background_subtraction_algorithm))\n",
    "    print(\"Data Augmentation - {}\\n\".format(parameters.data_augmentation))\n",
    "\n",
    "    # Printing Parameters\n",
    "    print(\n",
    "        \"Window Length = {}\\nStride = {}\\nFair Comparison = {}\\nDropout = {}\\nLearning Rate = {}\\nNum Epochs = {}\\nChunk Size = {}\\nForward Chunk = {}\\nForward Chunk Size = {}\\nLoss Fn = {}\\n\".format(\n",
    "            window_len,\n",
    "            stride,\n",
    "            fair_comparison,\n",
    "            dropout,\n",
    "            learning_rate,\n",
    "            num_epochs,\n",
    "            chunk_size,\n",
    "            forward_chunk,\n",
    "            forward_chunk_size,\n",
    "            loss_fn,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Training our model\n",
    "    def train_model(filepath):\n",
    "        print(\"Training has Begun\")\n",
    "        model.train()  # Sets the model in training mode.\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            val_loss = 0  # Not used\n",
    "            frame_stats = []  # Frame level statistics\n",
    "            window_stats = []  # Window level statistics\n",
    "\n",
    "            for i, (sample, labels) in enumerate(train_dataloader):\n",
    "                sample = sample.to(device, dtype=torch.float)  # Moves the input sample tensor to the specified device\n",
    "                chunks = torch.split(sample, chunk_size, dim=1)  # Splits the input sample into smaller chunks\n",
    "                recon_vid = []  # Empty list to store reconstructed video chunks\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    # ===================forward=====================\n",
    "                    output = model(chunk)  # Perform a forward pass of the model on the current chunk\n",
    "                    output = output.to(device).permute(1, 0, 2, 3, 4)  # Moves the output tensor to the device and permutes its dimensions. # fmt: skip\n",
    "                    model.zero_grad()  # Clears the gradients of the model parameters.\n",
    "                    loss = loss_fn(output, chunk)  # Computes the loss between the reconstructed output and the original chunk of data. # fmt: skip\n",
    "                    recon_vid.append(output)  # Appends the reconstructed output\n",
    "\n",
    "                    # ===================backward====================\n",
    "                    loss.backward()  # Getting gradients of the loss w.r.t. model parameters\n",
    "                    optimizer.step()  # Updating model parameters\n",
    "                    optimizer.zero_grad()  # Clear gradients of the model parameters for next iteration\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                if epoch == num_epochs - 1:\n",
    "                    output = torch.cat(recon_vid, dim=1)  # Concatenate reconstructed chunks\n",
    "                    # convert tensors to numpy arrays for easy manipluations\n",
    "                    sample = sample.data.cpu().numpy()\n",
    "                    output = output.data.cpu().numpy()\n",
    "                    labels = labels.data.cpu().numpy()\n",
    "                    frame_std, frame_mean, frame_labels, window_std, window_mean, window_labels = (\n",
    "                        get_performance_metrics(sample, output, labels, window_len)\n",
    "                    )\n",
    "                    frame_stats.append([frame_mean, frame_std, frame_labels])\n",
    "                    window_stats.append([window_mean, window_std, window_labels])\n",
    "\n",
    "            if epoch == num_epochs - 1:\n",
    "                # get_total_performance_metrics(frame_stats, window_stats, window_len)\n",
    "                recon_errors = []\n",
    "                recon_labels = []\n",
    "                for i in range(len(frame_stats)):\n",
    "                    # this is single video metrics\n",
    "                    frame_mean, frame_std, frame_labels = frame_stats[i]\n",
    "                    recon_errors.append([frame_mean, frame_std])\n",
    "                    recon_labels.append(frame_labels)\n",
    "                np.save(\n",
    "                    project_directory + \"\\Output\\Recon_Errors\\\\train_recon_errors_{}.npy\".format(modality),\n",
    "                    recon_errors,\n",
    "                )\n",
    "                np.save(\n",
    "                    project_directory + \"\\Output\\Recon_Errors\\\\train_recon_labels_{}.npy\".format(modality),\n",
    "                    recon_labels,\n",
    "                )\n",
    "\n",
    "            # ===================log========================\n",
    "            print(\"epoch [{}/{}], loss:{:.4f}\".format(epoch + 1, num_epochs, loss.item()))\n",
    "            torch.save(model.state_dict(), filepath)  # save the model each epoch at location filepath\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Training has Completed\\n\")\n",
    "\n",
    "    # Forward pass of model on test dataset\n",
    "    def forward_pass(path):\n",
    "        model.load_state_dict(torch.load(path))  # Load saved model weights\n",
    "        model.eval()  # Sets the model in testing mode.\n",
    "\n",
    "        frame_stats = []\n",
    "        window_stats = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            print(\"Forward pass occuring\")\n",
    "            for j, (sample, labels) in enumerate(test_dataloader):\n",
    "                # forward pass to get output\n",
    "                torch.cuda.empty_cache()\n",
    "                sample = sample.to(device, dtype=torch.float)\n",
    "                chunks = torch.split(sample, forward_chunk, dim=1)\n",
    "                recon_vid = []\n",
    "                for chunk in chunks:\n",
    "                    output = model(chunk)\n",
    "                    output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    recon_vid.append(output)\n",
    "                    torch.cuda.empty_cache()\n",
    "                output = torch.cat(recon_vid, dim=1)\n",
    "                # convert tensors to numpy arrays for easy manipluations\n",
    "                sample = sample.data.cpu().numpy()\n",
    "                output = output.data.cpu().numpy()\n",
    "                labels = labels.data.cpu().numpy()\n",
    "\n",
    "                frame_std, frame_mean, frame_labels, window_std, window_mean, window_labels = get_performance_metrics(\n",
    "                    sample, output, labels, window_len\n",
    "                )\n",
    "                frame_stats.append([frame_mean, frame_std, frame_labels])\n",
    "                window_stats.append([window_mean, window_std, window_labels])\n",
    "\n",
    "                # if j % 10 == 0:\n",
    "                # print(sample.shape)\n",
    "                # animate(sample[0, :, :, :, :], output[0, :, :, :, :], frame_mean, dset, start_time)\n",
    "\n",
    "            print(\"Forward pass completed\\n\")\n",
    "\n",
    "        return (frame_stats, window_stats)\n",
    "\n",
    "    start_time = str(datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "    modality = name + \"_\" + start_time\n",
    "    filepath = project_directory + \"\\Output\\Models\\\\\" + modality\n",
    "\n",
    "    # Comment out this call if you dont want to train a model\n",
    "    train_model(filepath)\n",
    "\n",
    "    # Insert any modelpath instead of filepath to use a specified pre trained model\n",
    "    frame_stats, window_stats = forward_pass(filepath)\n",
    "\n",
    "    print(\"{}\\n\".format(modality))\n",
    "    get_total_performance_metrics(modality, frame_stats, window_stats, window_len)\n",
    "    get_global_performance_metrics(modality, frame_stats, window_stats, window_len)\n",
    "\n",
    "    return ()\n",
    "\n",
    "\n",
    "# Directory names of the raw dataset from the Fall-Data folder\n",
    "# list_of_files = ['Thermal','ONI_IR','IP']\n",
    "list_of_files = [\"Thermal\"]\n",
    "\n",
    "# Dataset names used during H5PY file creation (dsets variable from dataset_creator.py)\n",
    "# list_of_datasets = ['Thermal_T3','ONI_IR_T','IP_T']\n",
    "list_of_datasets = [\"Thermal_T3\"]\n",
    "\n",
    "# List of pre-trained model weight location if wanting to test pre-trained model. 'x' should be replaced with path to model weight location\n",
    "# list_of_models = ['x','x','x','x','x','x']\n",
    "list_of_models = [\"x\"]\n",
    "\n",
    "for i in range(len(list_of_datasets)):\n",
    "    modelpath = list_of_models[i]\n",
    "    name = list_of_datasets[i]\n",
    "    dset = list_of_files[i]\n",
    "    path = \"{}\\Dataset\\H5PY\\Data_set-{}-imgdim64x64.h5\".format(project_directory, name)\n",
    "    full_pipeline(name, dset, window_len, fair_comparison, path, stride, modelpath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
